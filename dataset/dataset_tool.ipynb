{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Faster RCNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 600, 1200])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sebri\\anaconda3\\envs\\dataset\\lib\\site-packages\\torchvision\\ops\\boxes.py:101: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  ..\\torch\\csrc\\utils\\python_arg_parser.cpp:766.)\n",
      "  keep = keep.nonzero().squeeze(1)\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "# For training\n",
    "images, boxes = torch.rand(4, 3, 600, 1200), torch.rand(4, 11, 4)\n",
    "\n",
    "print(images[0].size())\n",
    "# For inference\n",
    "model.eval()\n",
    "predictions = model([images[0]])\n",
    "\n",
    "class_names = ['BG', 'person', 'bicycle', 'car', 'motorcycle', 'airplane',\n",
    "               'bus', 'train', 'truck', 'boat', 'traffic light',\n",
    "               'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird',\n",
    "               'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear',\n",
    "               'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie',\n",
    "               'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
    "               'kite', 'baseball bat', 'baseball glove', 'skateboard',\n",
    "               'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup',\n",
    "               'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n",
    "               'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
    "               'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed',\n",
    "               'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',\n",
    "               'keyboard', 'cell phone', 'microwave', 'oven', 'toaster',\n",
    "               'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors',\n",
    "               'teddy bear', 'hair drier', 'toothbrush']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███████████████████████▋                                                  | 1976/6164 [1:50:57<3:39:04,  3.14s/it]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "from torchvision.transforms import ToTensor\n",
    "from tqdm import tqdm\n",
    "import pathlib\n",
    "import pickle\n",
    "\n",
    "\n",
    "image_dir = './corgis'\n",
    "pred_dir = os.path.join(image_dir, 'predictions')\n",
    "pathlib.Path(pred_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "batch_size = 10\n",
    "image_names = os.listdir(image_dir)\n",
    "iters = len(image_names) // batch_size + 1\n",
    "\n",
    "for image_name in tqdm(os.listdir(image_dir)):\n",
    "    image = Image.open(os.path.join(image_dir, image_name))\n",
    "    image_tensor = ToTensor()(image).unsqueeze(0)\n",
    "    model.eval()\n",
    "    predictions = model(image_tensor)\n",
    "    pickle.dump(predictions[0], open(os.path.join(pred_dir, image_name.split('.')[0] + '.pkl'), \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_videos(image_dir):\n",
    "    file_names = os.listdir(image_dir)\n",
    "    for file_name in file_names:\n",
    "        if os.path.isdir(os.path.join(image_dir, file_name)):\n",
    "            continue\n",
    "        ext = file_name.split('.')\n",
    "        if len(ext) < 2 or ext[1] not in ['jpg', 'jpeg', 'png']:\n",
    "            os.remove(os.path.join(image_dir, file_name))\n",
    "            \n",
    "remove_videos(image_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|███████████████████████████████████████████████████████████████████████▌  | 13212/13657 [00:03<00:00, 3754.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asd\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 13657/13657 [00:03<00:00, 3755.39it/s]\n"
     ]
    }
   ],
   "source": [
    "def create_size_dict(image_dir):\n",
    "    image_names = os.listdir(image_dir)\n",
    "    size_dict = {}\n",
    "    for image_name in tqdm(image_names):\n",
    "        if os.path.isdir(os.path.join(image_dir, image_name)):\n",
    "            print('asd')\n",
    "            continue\n",
    "        \n",
    "        image = Image.open(os.path.join(image_dir, image_name))\n",
    "        w,h = image.size\n",
    "        size_str = f'{w}x{h}'\n",
    "        if size_str not in size_dict.keys():\n",
    "            size_dict[size_str] = [image_name]\n",
    "        else:\n",
    "            size_dict[size_str] = size_dict[size_str] + [image_name]\n",
    "            \n",
    "    return size_dict\n",
    "\n",
    "size_dict = create_size_dict(image_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6134\n"
     ]
    }
   ],
   "source": [
    "removed = 0\n",
    "remove_keys = []\n",
    "for size in size_dict.keys():\n",
    "    w, h = [int(x) for x in size.split('x')]\n",
    "    if w < 700 and h < 700:\n",
    "        for image_name in size_dict[size]:\n",
    "            os.remove(os.path.join(image_dir, image_name))\n",
    "            removed += 1\n",
    "        remove_keys.append(size)\n",
    "        \n",
    "for key in remove_keys:\n",
    "    size_dict.pop(key, None)\n",
    "\n",
    "print(removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1359\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "\n",
    "removed = 0\n",
    "for size in size_dict.keys():\n",
    "    if len(size_dict[size]) < 2:\n",
    "        continue\n",
    "    \n",
    "    hashes = []\n",
    "    image_files = []\n",
    "    for image_file in size_dict[size]:\n",
    "        hash_md5 = hashlib.md5()\n",
    "        with open(os.path.join(image_dir, image_file), \"rb\") as f:\n",
    "            for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "                hash_md5.update(chunk)\n",
    "        hashes.append(hash_md5.hexdigest())\n",
    "        image_files.append(image_file)\n",
    "        \n",
    "    unique_hashes = set(hashes)\n",
    "    for h in unique_hashes:\n",
    "        cnt = hashes.count(h)\n",
    "        if cnt > 1:\n",
    "            idxs = [i for i in range(len(hashes)) if hashes[i] == h]\n",
    "            idxs.pop(0)\n",
    "            for idx in idxs:\n",
    "                os.remove(os.path.join(image_dir, image_files[idx]))\n",
    "                removed +=1\n",
    "\n",
    "print(removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_dupes = [size for size in images if len(images[size]) > 1]\n",
    "for size in possible_dupes:\n",
    "    hashes = defaultdict(list)\n",
    "    for fname in images[size]:\n",
    "        m = md5.new()\n",
    "        hashes[ m.update( file(fname,'rb').read(10000) ).digest() ] = fname\n",
    "    for k in hashes:\n",
    "       if len(hashes[k]) <= 1: continue\n",
    "       for fname in hashes[k][1:]:\n",
    "           os.remove(fname)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
